# @package _global_

# specify here default configuration
# order of defaults determines the order in which configs override each other
defaults:
  - conflearn: overlook
  - paths: default
  - data: coco_seg
  - model: fcn
  - callbacks: default
  - logger: wandb # set logger here or use command line (e.g. `python train.py logger=tensorboard`)
  - trainer: gpu
  - _self_ # If the new behavior works for your application, append _self_ to the end of the Defaults List.
           # If your application requires the previous behavior, insert _self_ as the first item in your Defaults List. 

# seed for random number generators in pytorch, numpy and python.random
seed: 42
num_folds: 4
task_name: battery
num_epochs: 12
pooling: true
softmin_temperature: 0.1

paths:
  data_root_dir: /datasets/conflearn/seg/battery

data: 
  batch_size: 16
  train_transform:
    _target_: src.transforms.segmentation.SegmentationPresetTrain
    base_size: 520
    crop_size: 480
    hflip_prob: 0.5

  valid_transform:
    _target_: src.transforms.segmentation.SegmentationPresetEval

  test_transform:
    _target_: src.transforms.segmentation.SegmentationPresetEval

  noise_type: ${conflearn.noise_type}
  noise_config: ${conflearn.noise_config}

model:
  optimizer:
    lr: 0.001
    weight_decay: 0.0001
  
  scheduler:
    _target_: torch.optim.lr_scheduler.CosineAnnealingLR
    T_max: ${num_epochs}
    eta_min: 0
    last_epoch: -1
  
  compile: true
  aux_loss: true
  
logger:
  project: "confident-learning"
  tags: ["segmentation", "fcn", "${task_name}", "${data.noise_type}"]
  save_dir: ${paths.root_dir}/outputs

trainer:
  max_epochs: ${num_epochs}
  deterministic: false # for ignore_index in nn.CrossEntropyLoss() to work, deterministic should be False
  num_sanity_val_steps: 0
  precision: 16-mixed
  log_every_n_steps: 1
  fast_dev_run: false